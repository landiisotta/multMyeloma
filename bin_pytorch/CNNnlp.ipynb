{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for text data\n",
    "\n",
    "First of all we need to convert text into numerical representations. This process is called __vectorization__. It can be done:\n",
    "\n",
    "- __convert__ text into words and words into vectors;\n",
    "- __convert__ text into characters and characters into vectors;\n",
    "\n",
    "- create __n-gram__ of words and represent them as vectors.\n",
    "\n",
    "Each smaller unit of text is called __token__, hence the process of braking a text into tokens is called __tokenization__. The most popular approaches to convert a token into a vector are: _one-hot encoding_ and _word embedding_.\n",
    "\n",
    "In one-hot encoding, each token is represented as a vector of size $N$, where $N$ is the size of the vocabulary (i.e. total number of unique words in the document). The one-hot representation of a vocabulary has dimension $|V|\\times |V|$.\n",
    "\n",
    "Word embedding is a popular way of representing text data in problems solved by deep learning algorithms. Word embedding provides a representation of a word. The dimension of the vector is a hyper-parameter set during training phase. The representation of a vocabulary becomes $|V|\\times D$, where $D$ is the dimension to be set. One way to create word embeddings is to start with dense vectors for each token containing random numbers, and then train a model such as a document classifier. Hence, the numbers representing the tokens will get adjusted such that semantically closer words will have smaller vectorial distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization: split text into characters/words\n",
    "thorTxt = \"The action scenes were top notch in this movie.\\\n",
    "           Thor has never been this epic in the MCU. He does\\\n",
    "           some pretty epic shit in this movie and he is\\\n",
    "           definitely not under-powered anymore. Thor in\\\n",
    "           unleashed in this, I love that.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to express tokens as characters, we can use the `list()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thor_char = list(thorTxt)\n",
    "print(thor_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if we want to express tokens as words, we can use the `split()` function in the Python string object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thor_word = thorTxt.split()\n",
    "print(thor_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__N-grams__ are group of words extracted from given text. $N$ represents the number of words that can be used together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##n-gram representation N = 2\n",
    "\n",
    "thor_ngram = [(thor_word[i], thor_word[i+1]) for i in range(len(thor_word)-1)]\n",
    "print(thor_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##one-hot encoding\n",
    "##we first map each word in the vocabulary to a unique index\n",
    "word_to_ix = {}\n",
    "\n",
    "for word in thor_word:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_thor = torch.zeros(len(word_to_ix), len(word_to_ix)) ##onehot tensor of dimension |V|x|V|\n",
    "\n",
    "for word in word_to_ix:\n",
    "    id = word_to_ix[word]\n",
    "    onehot_thor[id, id] = 1 \n",
    "    \n",
    "onehot_w = torch.zeros(len(word_to_ix))\n",
    "\n",
    "def onehot(word, word_to_ix):\n",
    "    onehot_vect = torch.zeros(len(word_to_ix)) ##onehot tensor of dimension |V| for a word from the vocabulary \n",
    "    ix = word_to_ix[word]\n",
    "    onehot_vect[ix] = 1\n",
    "    return onehot_vect\n",
    "\n",
    "onehot('were', word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(onehot_thor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FINIRE!\n",
    "##Word embedding\n",
    "##training (Continuous Bag of Words: predicts word given the context) = pretraining embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "impotr numpy as np\n",
    "\n",
    "##create context tensor\n",
    "def make_contex_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor\n",
    "\n",
    "def get_index_max(input):\n",
    "    index = 0\n",
    "    for i in range(len(input), 1):\n",
    "        if input[i] > input[index]:\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "def get_max_prob_result(input, ix_to_word):\n",
    "    return ix_to_word[get_index_max(input)]\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation2 = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view((1, -1))\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation2(out)\n",
    "        return out\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        word = torch.LongTensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model.train()\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoc in range(50):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_vector)\n",
    "        loss = loss_function(log_probs, torch.LongTensor([word_to_ix[target]]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for text classification \n",
    "We create word embeddings as part of our architecture and train the entire model for prediction. CNN performes feature learning.\n",
    "\n",
    "We use `torchtext` to download the `IMDB` dataset and split it into `train` and `test` datasets. `torchtext` for downloading, tokenizing and building vocabulary for the `IMDB` dataset.\n",
    "The `torchtext` package consists of data processing utilities and popular datasets for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data preparation\n",
    "import torchtext\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT = data.Field(lower=True, batch_first=True, fix_length=20) ##lowercase the text, tokenize the text and trim it to a maximum length of 20\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL) ##LABEL: positive, negative review\n",
    "\n",
    "##now we build the vocabulary for the data\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300), \n",
    "                 max_size=10000, min_freq=10) ##pretrained embeddings of dim=300\n",
    "##max number of words in the vocab = 10000 --> one element has token = 10001!! WHY?\n",
    "##removed words with freq<10\n",
    "##GloVe=Global Vectors for word representation\n",
    "LABEL.build_vocab(train,)\n",
    "\n",
    "##create iterators that generate batches for train and test datasets\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test),\n",
    "                                                   batch_size=128, \n",
    "                                                   device=-1, \n",
    "                                                   shuffle=False)##device=-1 means CPU, None for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20])\n",
      "tensor([[ 2346,     0,     0,  ...,    13,     0,  2346],\n",
      "        [    9,  4923,     9,  ...,  1045,   158,  1308],\n",
      "        [ 3360,     0,    14,  ...,    25,    90,   337],\n",
      "        ...,\n",
      "        [  828,   872,  1679,  ...,     6,    48,    74],\n",
      "        [    3,    75,     5,  ...,     2,    49,  4725],\n",
      "        [    9,   120,   775,  ...,   248,  4342,     3]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter)) ##inspect one batch\n",
    "b = batch.text\n",
    "l = batch.label\n",
    "print(b.size()) #dim: 128x20 [batch_size, fix_len(from TEXT definition of sequence length)]\n",
    "#l.size()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "len(train_iter.dataset.examples)\n",
    "print(len(test_iter.dataset.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors) ##shows embeddings\n",
    "print(TEXT.vocab.freqs) ##shows frequencies of tokens\n",
    "print(TEXT.vocab.stoi) ##shows the dictionary with words and indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "##CNN implementation\n",
    "class IMDBcnn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_cat, bs=1, max_len=20, kernel_size=3):\n",
    "        super(IMDBcnn, self).__init__()\n",
    "        self.bs = bs\n",
    "        self.n_cat = n_cat\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) ##n_channels=tokens\n",
    "        self.cnn = nn.Conv1d(max_len, embedding_dim, kernel_size) ##one-dimensional convolution (time as a conv feat)\n",
    "        ##convolution layer accepts the sequence_length and the embedding_dimension\n",
    "        self.avg = nn.AdaptiveAvgPool1d(10)\n",
    "        self.fc = nn.Linear(1000, n_cat)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        bs = input.size()[0]\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "        embed = self.embedding(input)\n",
    "        out = self.cnn(embed)\n",
    "        out = self.avg(out)\n",
    "        out = out.view(self.bs, -1)\n",
    "        #print(out.size())\n",
    "        out = F.dropout(self.fc(out), p=0.5)\n",
    "        #print(out.size())\n",
    "        return self.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IMDBcnn(\n",
       "  (embedding): Embedding(20000, 100)\n",
       "  (cnn): Conv1d(20, 100, kernel_size=(3,), stride=(1,))\n",
       "  (avg): AdaptiveAvgPool1d(output_size=10)\n",
       "  (fc): Linear(in_features=1000, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = IMDBcnn(vocab_size=20000, embedding_dim=100, n_cat=2, bs=128)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.663877\n",
      "[1,  1001] loss: 0.873771\n",
      "[1,  2001] loss: 0.734582\n",
      "[1,  3001] loss: 0.660496\n",
      "[1,  4001] loss: 0.587941\n",
      "[1,  5001] loss: 0.707496\n",
      "[1,  6001] loss: 0.706104\n",
      "[1,  7001] loss: 0.641321\n",
      "[1,  8001] loss: 0.617158\n",
      "[1,  9001] loss: 0.453076\n",
      "[1, 10001] loss: 0.606310\n",
      "[1, 11001] loss: 0.585477\n",
      "[1, 12001] loss: 0.478372\n",
      "[1, 13001] loss: 0.443530\n",
      "[1, 14001] loss: 0.411518\n",
      "[1, 15001] loss: 0.571858\n",
      "[1, 16001] loss: 0.431129\n",
      "[1, 17001] loss: 0.352848\n",
      "[1, 18001] loss: 0.339925\n",
      "[1, 19001] loss: 0.294157\n",
      "[1, 20001] loss: 0.430539\n",
      "[1, 21001] loss: 0.408975\n",
      "[1, 22001] loss: 0.405468\n",
      "[1, 23001] loss: 0.249852\n",
      "[1, 24001] loss: 0.240808\n",
      "[1, 25001] loss: 0.333480\n",
      "[1, 26001] loss: 0.392163\n",
      "[1, 27001] loss: 0.353186\n",
      "[1, 28001] loss: 0.220064\n",
      "[1, 29001] loss: 0.221836\n",
      "[1, 30001] loss: 0.299842\n",
      "[1, 31001] loss: 0.209823\n",
      "[1, 32001] loss: 0.184576\n",
      "[1, 33001] loss: 0.218767\n",
      "[1, 34001] loss: 0.170951\n",
      "[1, 35001] loss: 0.335098\n",
      "[1, 36001] loss: 0.198010\n",
      "[1, 37001] loss: 0.138783\n",
      "[1, 38001] loss: 0.144393\n",
      "[1, 39001] loss: 0.066589\n",
      "[1, 40001] loss: 0.172500\n",
      "[1, 41001] loss: 0.101866\n",
      "[1, 42001] loss: 0.079332\n",
      "[1, 43001] loss: 0.087928\n",
      "[1, 44001] loss: 0.034551\n",
      "[1, 45001] loss: 0.054960\n",
      "[1, 46001] loss: 0.075356\n",
      "[1, 47001] loss: 0.020420\n",
      "[1, 48001] loss: 0.032165\n",
      "[1, 49001] loss: 0.016809\n",
      "[1, 50001] loss: 0.041091\n",
      "[1, 51001] loss: 0.019916\n",
      "[1, 52001] loss: 0.025651\n",
      "[1, 53001] loss: 0.010152\n",
      "[1, 54001] loss: 0.005070\n",
      "[1, 55001] loss: 0.024774\n",
      "[1, 56001] loss: 0.015157\n",
      "[1, 57001] loss: 0.006435\n",
      "[1, 58001] loss: 0.005058\n",
      "[1, 59001] loss: 0.002632\n",
      "[1, 60001] loss: 0.003207\n",
      "[1, 61001] loss: 0.001935\n",
      "[1, 62001] loss: 0.003021\n",
      "[1, 63001] loss: 0.000807\n",
      "[1, 64001] loss: 0.000511\n",
      "[1, 65001] loss: 0.000459\n",
      "[1, 66001] loss: 0.006441\n",
      "[1, 67001] loss: 0.000322\n",
      "[1, 68001] loss: 0.000157\n",
      "[1, 69001] loss: 0.000110\n",
      "[1, 70001] loss: 0.000103\n",
      "[1, 71001] loss: 0.000097\n",
      "[1, 72001] loss: 0.000113\n",
      "[1, 73001] loss: 0.000097\n",
      "[1, 74001] loss: 0.000044\n",
      "[1, 75001] loss: 0.000070\n",
      "[1, 76001] loss: 0.003833\n",
      "[1, 77001] loss: 0.000029\n",
      "[1, 78001] loss: 0.000039\n",
      "[1, 79001] loss: 0.000033\n",
      "[1, 80001] loss: 0.000026\n",
      "[1, 81001] loss: 0.000018\n",
      "[1, 82001] loss: 0.000021\n",
      "[1, 83001] loss: 0.000027\n",
      "[1, 84001] loss: 0.000017\n",
      "[1, 85001] loss: 0.000012\n",
      "[1, 86001] loss: 0.000010\n",
      "[1, 87001] loss: 0.000014\n",
      "[1, 88001] loss: 0.000006\n",
      "[1, 89001] loss: 0.000009\n",
      "[1, 90001] loss: 0.000006\n",
      "[1, 91001] loss: 0.000007\n",
      "[1, 92001] loss: 0.000004\n",
      "[1, 93001] loss: 0.000007\n",
      "[1, 94001] loss: 0.000004\n",
      "[1, 95001] loss: 0.000005\n",
      "[1, 96001] loss: 0.000003\n",
      "[1, 97001] loss: 0.000004\n",
      "[1, 98001] loss: 0.000004\n",
      "[1, 99001] loss: 0.000004\n",
      "[1, 100001] loss: 0.000004\n",
      "[1, 101001] loss: 0.000004\n",
      "[1, 102001] loss: 0.000002\n",
      "[1, 103001] loss: 0.000002\n",
      "[1, 104001] loss: 0.000003\n",
      "[1, 105001] loss: 0.000003\n",
      "[1, 106001] loss: 0.000002\n",
      "[1, 107001] loss: 0.000003\n",
      "[1, 108001] loss: 0.000002\n",
      "[1, 109001] loss: 0.000003\n",
      "[1, 110001] loss: 0.000002\n",
      "[1, 111001] loss: 0.000002\n",
      "[1, 112001] loss: 0.000002\n",
      "[1, 113001] loss: 0.000002\n",
      "[1, 114001] loss: 0.000002\n",
      "[1, 115001] loss: 0.006051\n",
      "[1, 116001] loss: 0.000002\n",
      "[1, 117001] loss: 0.000001\n",
      "[1, 118001] loss: 0.000001\n",
      "[1, 119001] loss: 0.000001\n",
      "[1, 120001] loss: 0.000001\n",
      "[1, 121001] loss: 0.000001\n",
      "[1, 122001] loss: 0.000002\n",
      "[1, 123001] loss: 0.000001\n",
      "[1, 124001] loss: 0.000002\n",
      "[1, 125001] loss: 0.003796\n",
      "[1, 126001] loss: 0.000001\n",
      "[1, 127001] loss: 0.000001\n",
      "[1, 128001] loss: 0.000001\n",
      "[1, 129001] loss: 0.000001\n",
      "[1, 130001] loss: 0.000001\n",
      "[1, 131001] loss: 0.000001\n",
      "[1, 132001] loss: 0.000002\n",
      "[1, 133001] loss: 0.000001\n",
      "[1, 134001] loss: 0.000001\n",
      "[1, 135001] loss: 0.000001\n",
      "[1, 136001] loss: 0.000001\n",
      "[1, 137001] loss: 0.000001\n",
      "[1, 138001] loss: 0.000001\n",
      "[1, 139001] loss: 0.000001\n",
      "[1, 140001] loss: 0.000001\n",
      "[1, 141001] loss: 0.000001\n",
      "[1, 142001] loss: 0.000001\n",
      "[1, 143001] loss: 0.000001\n",
      "[1, 144001] loss: 0.000001\n",
      "[1, 145001] loss: 0.000001\n",
      "[1, 146001] loss: 0.000001\n",
      "[1, 147001] loss: 0.000001\n",
      "[1, 148001] loss: 0.000001\n",
      "[1, 149001] loss: 0.000001\n",
      "[1, 150001] loss: 0.000001\n",
      "[1, 151001] loss: 0.000001\n",
      "[1, 152001] loss: 0.000001\n",
      "[1, 153001] loss: 0.000001\n",
      "[1, 154001] loss: 0.000001\n",
      "[1, 155001] loss: 0.000001\n",
      "[1, 156001] loss: 0.000001\n",
      "[1, 157001] loss: 0.000001\n",
      "[1, 158001] loss: 0.000001\n",
      "[1, 159001] loss: 0.000001\n",
      "[1, 160001] loss: 0.000001\n",
      "[1, 161001] loss: 0.000001\n",
      "[1, 162001] loss: 0.000001\n",
      "[1, 163001] loss: 0.000001\n",
      "[1, 164001] loss: 0.005888\n",
      "[1, 165001] loss: 0.000001\n",
      "[1, 166001] loss: 0.000001\n",
      "[1, 167001] loss: 0.000001\n",
      "[1, 168001] loss: 0.000001\n",
      "[1, 169001] loss: 0.000001\n",
      "[1, 170001] loss: 0.000001\n",
      "[1, 171001] loss: 0.000001\n",
      "[1, 172001] loss: 0.000001\n",
      "[1, 173001] loss: 0.000001\n",
      "[1, 174001] loss: 0.003814\n",
      "[1, 175001] loss: 0.000001\n",
      "[1, 176001] loss: 0.000001\n",
      "[1, 177001] loss: 0.000001\n",
      "[1, 178001] loss: 0.000001\n",
      "[1, 179001] loss: 0.000001\n",
      "[1, 180001] loss: 0.000001\n",
      "[1, 181001] loss: 0.000001\n",
      "[1, 182001] loss: 0.000001\n",
      "[1, 183001] loss: 0.000001\n",
      "[1, 184001] loss: 0.000001\n",
      "[1, 185001] loss: 0.000001\n",
      "[1, 186001] loss: 0.000001\n",
      "[1, 187001] loss: 0.000001\n",
      "[1, 188001] loss: 0.000001\n",
      "[1, 189001] loss: 0.000001\n",
      "[1, 190001] loss: 0.000001\n",
      "[1, 191001] loss: 0.000001\n",
      "[1, 192001] loss: 0.000001\n",
      "[1, 193001] loss: 0.000001\n",
      "[1, 194001] loss: 0.000001\n",
      "[1, 195001] loss: 0.000001\n",
      "[1, 196001] loss: 0.000001\n",
      "[1, 197001] loss: 0.000001\n",
      "[1, 198001] loss: 0.000001\n",
      "[1, 199001] loss: 0.000001\n",
      "[1, 200001] loss: 0.000001\n",
      "[1, 201001] loss: 0.000001\n",
      "[1, 202001] loss: 0.000001\n",
      "[1, 203001] loss: 0.000001\n",
      "[1, 204001] loss: 0.000001\n",
      "[1, 205001] loss: 0.000001\n",
      "[1, 206001] loss: 0.000001\n",
      "[1, 207001] loss: 0.000001\n",
      "[1, 208001] loss: 0.000001\n",
      "[1, 209001] loss: 0.000001\n",
      "[1, 210001] loss: 0.000001\n",
      "[1, 211001] loss: 0.000001\n",
      "[1, 212001] loss: 0.000001\n",
      "[1, 213001] loss: 0.005727\n",
      "[1, 214001] loss: 0.000001\n",
      "[1, 215001] loss: 0.000001\n",
      "[1, 216001] loss: 0.000001\n",
      "[1, 217001] loss: 0.000001\n",
      "[1, 218001] loss: 0.000001\n",
      "[1, 219001] loss: 0.000001\n",
      "[1, 220001] loss: 0.000001\n",
      "[1, 221001] loss: 0.000001\n",
      "[1, 222001] loss: 0.000001\n",
      "[1, 223001] loss: 0.003825\n",
      "[1, 224001] loss: 0.000001\n",
      "[1, 225001] loss: 0.000001\n",
      "[1, 226001] loss: 0.000001\n",
      "[1, 227001] loss: 0.000001\n",
      "[1, 228001] loss: 0.000001\n",
      "[1, 229001] loss: 0.000001\n",
      "[1, 230001] loss: 0.000001\n",
      "[1, 231001] loss: 0.000001\n",
      "[1, 232001] loss: 0.000001\n",
      "[1, 233001] loss: 0.000001\n",
      "[1, 234001] loss: 0.000001\n",
      "[1, 235001] loss: 0.000001\n",
      "[1, 236001] loss: 0.000001\n",
      "[1, 237001] loss: 0.000001\n",
      "[1, 238001] loss: 0.000001\n",
      "[1, 239001] loss: 0.000001\n",
      "[1, 240001] loss: 0.000001\n",
      "[1, 241001] loss: 0.000001\n",
      "[1, 242001] loss: 0.000001\n",
      "[1, 243001] loss: 0.000001\n",
      "[1, 244001] loss: 0.000001\n",
      "[1, 245001] loss: 0.000001\n",
      "[1, 246001] loss: 0.000001\n",
      "[1, 247001] loss: 0.000001\n",
      "[1, 248001] loss: 0.000001\n",
      "[1, 249001] loss: 0.000001\n",
      "[1, 250001] loss: 0.000001\n",
      "[1, 251001] loss: 0.000001\n",
      "[1, 252001] loss: 0.000001\n",
      "[1, 253001] loss: 0.000001\n",
      "[1, 254001] loss: 0.000001\n",
      "[1, 255001] loss: 0.000001\n",
      "[1, 256001] loss: 0.000001\n",
      "[1, 257001] loss: 0.000001\n",
      "[1, 258001] loss: 0.000001\n",
      "[1, 259001] loss: 0.000001\n",
      "[1, 260001] loss: 0.000001\n",
      "[1, 261001] loss: 0.000001\n",
      "[1, 262001] loss: 0.005570\n",
      "[1, 263001] loss: 0.000001\n",
      "[1, 264001] loss: 0.000001\n",
      "[1, 265001] loss: 0.000001\n",
      "[1, 266001] loss: 0.000001\n",
      "[1, 267001] loss: 0.000001\n",
      "[1, 268001] loss: 0.000001\n",
      "[1, 269001] loss: 0.000001\n",
      "[1, 270001] loss: 0.000001\n",
      "[1, 271001] loss: 0.000001\n",
      "[1, 272001] loss: 0.003829\n",
      "[1, 273001] loss: 0.000001\n",
      "[1, 274001] loss: 0.000001\n",
      "[1, 275001] loss: 0.000001\n",
      "[1, 276001] loss: 0.000001\n",
      "[1, 277001] loss: 0.000001\n",
      "[1, 278001] loss: 0.000001\n",
      "[1, 279001] loss: 0.000001\n",
      "[1, 280001] loss: 0.000001\n",
      "[1, 281001] loss: 0.000001\n",
      "[1, 282001] loss: 0.000001\n",
      "[1, 283001] loss: 0.000001\n",
      "[1, 284001] loss: 0.000001\n",
      "[1, 285001] loss: 0.000001\n",
      "[1, 286001] loss: 0.000001\n",
      "[1, 287001] loss: 0.000001\n",
      "[1, 288001] loss: 0.000001\n",
      "[1, 289001] loss: 0.000001\n",
      "[1, 290001] loss: 0.000001\n",
      "[1, 291001] loss: 0.000001\n",
      "[1, 292001] loss: 0.000001\n",
      "[1, 293001] loss: 0.000001\n",
      "[1, 294001] loss: 0.000001\n",
      "[1, 295001] loss: 0.000001\n",
      "[1, 296001] loss: 0.000001\n",
      "[1, 297001] loss: 0.000001\n",
      "[1, 298001] loss: 0.000001\n",
      "[1, 299001] loss: 0.000001\n",
      "[1, 300001] loss: 0.000001\n",
      "[1, 301001] loss: 0.000001\n",
      "[1, 302001] loss: 0.000001\n",
      "[1, 303001] loss: 0.000001\n",
      "[1, 304001] loss: 0.000001\n",
      "[1, 305001] loss: 0.000001\n",
      "[1, 306001] loss: 0.000001\n",
      "[1, 307001] loss: 0.000001\n",
      "[1, 308001] loss: 0.000001\n",
      "[1, 309001] loss: 0.000001\n",
      "[1, 310001] loss: 0.000001\n",
      "[1, 311001] loss: 0.005415\n",
      "[1, 312001] loss: 0.000001\n",
      "[1, 313001] loss: 0.000001\n",
      "[1, 314001] loss: 0.000001\n",
      "[1, 315001] loss: 0.000001\n",
      "[1, 316001] loss: 0.000001\n",
      "[1, 317001] loss: 0.000001\n",
      "[1, 318001] loss: 0.000001\n",
      "[1, 319001] loss: 0.000001\n",
      "[1, 320001] loss: 0.000001\n",
      "[1, 321001] loss: 0.003823\n",
      "[1, 322001] loss: 0.000001\n",
      "[1, 323001] loss: 0.000001\n",
      "[1, 324001] loss: 0.000001\n",
      "[1, 325001] loss: 0.000001\n",
      "[1, 326001] loss: 0.000001\n",
      "[1, 327001] loss: 0.000001\n",
      "[1, 328001] loss: 0.000001\n",
      "[1, 329001] loss: 0.000001\n",
      "[1, 330001] loss: 0.000001\n",
      "[1, 331001] loss: 0.000001\n",
      "[1, 332001] loss: 0.000001\n",
      "[1, 333001] loss: 0.000001\n",
      "[1, 334001] loss: 0.000001\n",
      "[1, 335001] loss: 0.000001\n",
      "[1, 336001] loss: 0.000001\n",
      "[1, 337001] loss: 0.000001\n",
      "[1, 338001] loss: 0.000001\n",
      "[1, 339001] loss: 0.000001\n",
      "[1, 340001] loss: 0.000001\n",
      "[1, 341001] loss: 0.000001\n",
      "[1, 342001] loss: 0.000001\n",
      "[1, 343001] loss: 0.000001\n",
      "[1, 344001] loss: 0.000001\n",
      "[1, 345001] loss: 0.000001\n",
      "[1, 346001] loss: 0.000001\n",
      "[1, 347001] loss: 0.000001\n",
      "[1, 348001] loss: 0.000001\n",
      "[1, 349001] loss: 0.000001\n",
      "[1, 350001] loss: 0.000001\n",
      "[1, 351001] loss: 0.000001\n",
      "[1, 352001] loss: 0.000001\n",
      "[1, 353001] loss: 0.000001\n",
      "[1, 354001] loss: 0.000001\n",
      "[1, 355001] loss: 0.000001\n",
      "[1, 356001] loss: 0.000001\n",
      "[1, 357001] loss: 0.000001\n",
      "[1, 358001] loss: 0.000001\n",
      "[1, 359001] loss: 0.000001\n",
      "[1, 360001] loss: 0.005263\n",
      "[1, 361001] loss: 0.000001\n",
      "[1, 362001] loss: 0.000001\n",
      "[1, 363001] loss: 0.000001\n",
      "[1, 364001] loss: 0.000001\n",
      "[1, 365001] loss: 0.000001\n",
      "[1, 366001] loss: 0.000001\n",
      "[1, 367001] loss: 0.000001\n",
      "[1, 368001] loss: 0.000001\n",
      "[1, 369001] loss: 0.000001\n",
      "[1, 370001] loss: 0.003810\n",
      "[1, 371001] loss: 0.000001\n",
      "[1, 372001] loss: 0.000001\n",
      "[1, 373001] loss: 0.000001\n",
      "[1, 374001] loss: 0.000001\n",
      "[1, 375001] loss: 0.000001\n",
      "[1, 376001] loss: 0.000001\n",
      "[1, 377001] loss: 0.000002\n",
      "[1, 378001] loss: 0.000001\n",
      "[1, 379001] loss: 0.000001\n",
      "[1, 380001] loss: 0.000001\n",
      "[1, 381001] loss: 0.000002\n",
      "[1, 382001] loss: 0.000001\n",
      "[1, 383001] loss: 0.000001\n",
      "[1, 384001] loss: 0.000001\n",
      "[1, 385001] loss: 0.000001\n",
      "[1, 386001] loss: 0.000001\n",
      "[1, 387001] loss: 0.000001\n",
      "[1, 388001] loss: 0.000001\n",
      "[1, 389001] loss: 0.000001\n",
      "[1, 390001] loss: 0.000001\n",
      "[1, 391001] loss: 0.000001\n",
      "[1, 392001] loss: 0.000001\n",
      "[1, 393001] loss: 0.000001\n",
      "[1, 394001] loss: 0.000001\n",
      "[1, 395001] loss: 0.000001\n",
      "[1, 396001] loss: 0.000001\n",
      "[1, 397001] loss: 0.000001\n",
      "[1, 398001] loss: 0.000001\n",
      "[1, 399001] loss: 0.000001\n",
      "[1, 400001] loss: 0.000001\n",
      "[1, 401001] loss: 0.000002\n",
      "[1, 402001] loss: 0.000001\n",
      "[1, 403001] loss: 0.000001\n",
      "[1, 404001] loss: 0.000001\n",
      "[1, 405001] loss: 0.000001\n",
      "[1, 406001] loss: 0.000001\n",
      "[1, 407001] loss: 0.000002\n",
      "[1, 408001] loss: 0.000001\n",
      "[1, 409001] loss: 0.005118\n",
      "[1, 410001] loss: 0.000001\n",
      "[1, 411001] loss: 0.000001\n",
      "[1, 412001] loss: 0.000001\n",
      "[1, 413001] loss: 0.000001\n",
      "[1, 414001] loss: 0.000002\n",
      "[1, 415001] loss: 0.000001\n",
      "[1, 416001] loss: 0.000002\n",
      "[1, 417001] loss: 0.000001\n",
      "[1, 418001] loss: 0.000002\n",
      "[1, 419001] loss: 0.003793\n",
      "[1, 420001] loss: 0.000001\n",
      "[1, 421001] loss: 0.000002\n",
      "[1, 422001] loss: 0.000002\n",
      "[1, 423001] loss: 0.000001\n",
      "[1, 424001] loss: 0.000001\n",
      "[1, 425001] loss: 0.000002\n",
      "[1, 426001] loss: 0.000002\n",
      "[1, 427001] loss: 0.000002\n",
      "[1, 428001] loss: 0.000002\n",
      "[1, 429001] loss: 0.000001\n",
      "[1, 430001] loss: 0.000002\n",
      "[1, 431001] loss: 0.000001\n",
      "[1, 432001] loss: 0.000002\n",
      "[1, 433001] loss: 0.000001\n",
      "[1, 434001] loss: 0.000001\n",
      "[1, 435001] loss: 0.000001\n",
      "[1, 436001] loss: 0.000002\n",
      "[1, 437001] loss: 0.000001\n",
      "[1, 438001] loss: 0.000001\n",
      "[1, 439001] loss: 0.000001\n",
      "[1, 440001] loss: 0.000002\n",
      "[1, 441001] loss: 0.000001\n",
      "[1, 442001] loss: 0.000002\n",
      "[1, 443001] loss: 0.000002\n",
      "[1, 444001] loss: 0.000002\n",
      "[1, 445001] loss: 0.000001\n",
      "[1, 446001] loss: 0.000001\n",
      "[1, 447001] loss: 0.000002\n",
      "[1, 448001] loss: 0.000002\n",
      "[1, 449001] loss: 0.000002\n",
      "[1, 450001] loss: 0.000002\n",
      "[1, 451001] loss: 0.000001\n",
      "[1, 452001] loss: 0.000002\n",
      "[1, 453001] loss: 0.000001\n",
      "[1, 454001] loss: 0.000001\n",
      "[1, 455001] loss: 0.000001\n",
      "[1, 456001] loss: 0.000002\n",
      "[1, 457001] loss: 0.000002\n",
      "[1, 458001] loss: 0.004976\n",
      "[1, 459001] loss: 0.000002\n",
      "[1, 460001] loss: 0.000001\n",
      "[1, 461001] loss: 0.000001\n",
      "[1, 462001] loss: 0.000001\n",
      "[1, 463001] loss: 0.000002\n",
      "[1, 464001] loss: 0.000001\n",
      "[1, 465001] loss: 0.000002\n",
      "[1, 466001] loss: 0.000001\n",
      "[1, 467001] loss: 0.000002\n",
      "[1, 468001] loss: 0.003770\n",
      "[1, 469001] loss: 0.000002\n",
      "[1, 470001] loss: 0.000002\n",
      "[1, 471001] loss: 0.000002\n",
      "[1, 472001] loss: 0.000002\n",
      "[1, 473001] loss: 0.000002\n",
      "[1, 474001] loss: 0.000002\n",
      "[1, 475001] loss: 0.000002\n",
      "[1, 476001] loss: 0.000002\n",
      "[1, 477001] loss: 0.000002\n",
      "[1, 478001] loss: 0.000002\n",
      "[1, 479001] loss: 0.000002\n",
      "[1, 480001] loss: 0.000001\n",
      "[1, 481001] loss: 0.000002\n",
      "[1, 482001] loss: 0.000002\n",
      "[1, 483001] loss: 0.000002\n",
      "[1, 484001] loss: 0.000001\n",
      "[1, 485001] loss: 0.000002\n",
      "[1, 486001] loss: 0.000002\n",
      "[1, 487001] loss: 0.000002\n",
      "[1, 488001] loss: 0.000001\n",
      "[1, 489001] loss: 0.000002\n",
      "[1, 490001] loss: 0.000001\n",
      "[1, 491001] loss: 0.000002\n",
      "[1, 492001] loss: 0.000002\n",
      "[1, 493001] loss: 0.000002\n",
      "[1, 494001] loss: 0.000001\n",
      "[1, 495001] loss: 0.000001\n",
      "[1, 496001] loss: 0.000002\n",
      "[1, 497001] loss: 0.000002\n",
      "[1, 498001] loss: 0.000002\n",
      "[1, 499001] loss: 0.000002\n",
      "[1, 500001] loss: 0.000002\n",
      "[1, 501001] loss: 0.000002\n",
      "[1, 502001] loss: 0.000001\n",
      "[1, 503001] loss: 0.000002\n",
      "[1, 504001] loss: 0.000001\n",
      "[1, 505001] loss: 0.000002\n",
      "[1, 506001] loss: 0.000002\n",
      "[1, 507001] loss: 0.004840\n",
      "[1, 508001] loss: 0.000002\n",
      "[1, 509001] loss: 0.000002\n",
      "[1, 510001] loss: 0.000001\n",
      "[1, 511001] loss: 0.000002\n",
      "[1, 512001] loss: 0.000002\n",
      "[1, 513001] loss: 0.000002\n",
      "[1, 514001] loss: 0.000002\n",
      "[1, 515001] loss: 0.000002\n",
      "[1, 516001] loss: 0.000002\n",
      "[1, 517001] loss: 0.003743\n",
      "[1, 518001] loss: 0.000002\n",
      "[1, 519001] loss: 0.000002\n",
      "[1, 520001] loss: 0.000002\n",
      "[1, 521001] loss: 0.000002\n",
      "[1, 522001] loss: 0.000002\n",
      "[1, 523001] loss: 0.000002\n",
      "[1, 524001] loss: 0.000002\n",
      "[1, 525001] loss: 0.000002\n",
      "[1, 526001] loss: 0.000002\n",
      "[1, 527001] loss: 0.000002\n",
      "[1, 528001] loss: 0.000002\n",
      "[1, 529001] loss: 0.000001\n",
      "[1, 530001] loss: 0.000002\n",
      "[1, 531001] loss: 0.000002\n",
      "[1, 532001] loss: 0.000002\n",
      "[1, 533001] loss: 0.000001\n",
      "[1, 534001] loss: 0.000002\n",
      "[1, 535001] loss: 0.000002\n",
      "[1, 536001] loss: 0.000002\n",
      "[1, 537001] loss: 0.000001\n",
      "[1, 538001] loss: 0.000002\n",
      "[1, 539001] loss: 0.000002\n",
      "[1, 540001] loss: 0.000002\n",
      "[1, 541001] loss: 0.000002\n",
      "[1, 542001] loss: 0.000002\n",
      "[1, 543001] loss: 0.000001\n",
      "[1, 544001] loss: 0.000002\n",
      "[1, 545001] loss: 0.000002\n",
      "[1, 546001] loss: 0.000002\n",
      "[1, 547001] loss: 0.000002\n",
      "[1, 548001] loss: 0.000002\n",
      "[1, 549001] loss: 0.000002\n",
      "[1, 550001] loss: 0.000002\n",
      "[1, 551001] loss: 0.000002\n",
      "[1, 552001] loss: 0.000002\n",
      "[1, 553001] loss: 0.000002\n",
      "[1, 554001] loss: 0.000002\n",
      "[1, 555001] loss: 0.000002\n",
      "[1, 556001] loss: 0.004719\n",
      "[1, 557001] loss: 0.000002\n",
      "[1, 558001] loss: 0.000002\n",
      "[1, 559001] loss: 0.000002\n",
      "[1, 560001] loss: 0.000002\n",
      "[1, 561001] loss: 0.000002\n",
      "[1, 562001] loss: 0.000002\n",
      "[1, 563001] loss: 0.000002\n",
      "[1, 564001] loss: 0.000002\n",
      "[1, 565001] loss: 0.000002\n",
      "[1, 566001] loss: 0.003716\n",
      "[1, 567001] loss: 0.000002\n",
      "[1, 568001] loss: 0.000002\n",
      "[1, 569001] loss: 0.000002\n",
      "[1, 570001] loss: 0.000002\n",
      "[1, 571001] loss: 0.000002\n",
      "[1, 572001] loss: 0.000002\n",
      "[1, 573001] loss: 0.000003\n",
      "[1, 574001] loss: 0.000002\n",
      "[1, 575001] loss: 0.000002\n",
      "[1, 576001] loss: 0.000002\n",
      "[1, 577001] loss: 0.000002\n",
      "[1, 578001] loss: 0.000001\n",
      "[1, 579001] loss: 0.000002\n",
      "[1, 580001] loss: 0.000002\n",
      "[1, 581001] loss: 0.000002\n",
      "[1, 582001] loss: 0.000002\n",
      "[1, 583001] loss: 0.000002\n",
      "[1, 584001] loss: 0.000002\n",
      "[1, 585001] loss: 0.000002\n",
      "[1, 586001] loss: 0.000002\n",
      "[1, 587001] loss: 0.000002\n",
      "[1, 588001] loss: 0.000002\n",
      "[1, 589001] loss: 0.000002\n",
      "[1, 590001] loss: 0.000002\n",
      "[1, 591001] loss: 0.000002\n",
      "[1, 592001] loss: 0.000002\n",
      "[1, 593001] loss: 0.000002\n",
      "[1, 594001] loss: 0.000002\n",
      "[1, 595001] loss: 0.000002\n",
      "[1, 596001] loss: 0.000002\n",
      "[1, 597001] loss: 0.000002\n",
      "[1, 598001] loss: 0.000002\n",
      "[1, 599001] loss: 0.000002\n",
      "[1, 600001] loss: 0.000002\n",
      "[1, 601001] loss: 0.000002\n",
      "[1, 602001] loss: 0.000002\n",
      "[1, 603001] loss: 0.000002\n",
      "[1, 604001] loss: 0.000002\n",
      "[1, 605001] loss: 0.004610\n",
      "[1, 606001] loss: 0.000002\n",
      "[1, 607001] loss: 0.000002\n",
      "[1, 608001] loss: 0.000002\n",
      "[1, 609001] loss: 0.000002\n",
      "[1, 610001] loss: 0.000002\n",
      "[1, 611001] loss: 0.000002\n",
      "[1, 612001] loss: 0.000002\n",
      "[1, 613001] loss: 0.000002\n",
      "[1, 614001] loss: 0.000002\n",
      "[1, 615001] loss: 0.003689\n",
      "[1, 616001] loss: 0.000002\n",
      "[1, 617001] loss: 0.000002\n",
      "[1, 618001] loss: 0.000002\n",
      "[1, 619001] loss: 0.000002\n",
      "[1, 620001] loss: 0.000002\n",
      "[1, 621001] loss: 0.000002\n",
      "[1, 622001] loss: 0.000003\n",
      "[1, 623001] loss: 0.000002\n",
      "[1, 624001] loss: 0.000002\n",
      "[1, 625001] loss: 0.000002\n",
      "[1, 626001] loss: 0.000003\n",
      "[1, 627001] loss: 0.000002\n",
      "[1, 628001] loss: 0.000002\n",
      "[1, 629001] loss: 0.000002\n",
      "[1, 630001] loss: 0.000002\n",
      "[1, 631001] loss: 0.000002\n",
      "[1, 632001] loss: 0.000003\n",
      "[1, 633001] loss: 0.000002\n",
      "[1, 634001] loss: 0.000002\n",
      "[1, 635001] loss: 0.000002\n",
      "[1, 636001] loss: 0.000002\n",
      "[1, 637001] loss: 0.000002\n",
      "[1, 638001] loss: 0.000002\n",
      "[1, 639001] loss: 0.000002\n",
      "[1, 640001] loss: 0.000002\n",
      "[1, 641001] loss: 0.000002\n",
      "[1, 642001] loss: 0.000002\n",
      "[1, 643001] loss: 0.000002\n",
      "[1, 644001] loss: 0.000002\n",
      "[1, 645001] loss: 0.000002\n",
      "[1, 646001] loss: 0.000002\n",
      "[1, 647001] loss: 0.000002\n",
      "[1, 648001] loss: 0.000003\n",
      "[1, 649001] loss: 0.000002\n",
      "[1, 650001] loss: 0.000002\n",
      "[1, 651001] loss: 0.000002\n",
      "[1, 652001] loss: 0.000002\n",
      "[1, 653001] loss: 0.000002\n",
      "[1, 654001] loss: 0.004510\n",
      "[1, 655001] loss: 0.000002\n",
      "[1, 656001] loss: 0.000002\n",
      "[1, 657001] loss: 0.000002\n",
      "[1, 658001] loss: 0.000002\n",
      "[1, 659001] loss: 0.000002\n",
      "[1, 660001] loss: 0.000002\n",
      "[1, 661001] loss: 0.000002\n",
      "[1, 662001] loss: 0.000002\n",
      "[1, 663001] loss: 0.000002\n",
      "[1, 664001] loss: 0.003662\n",
      "[1, 665001] loss: 0.000002\n",
      "[1, 666001] loss: 0.000003\n",
      "[1, 667001] loss: 0.000002\n",
      "[1, 668001] loss: 0.000002\n",
      "[1, 669001] loss: 0.000002\n",
      "[1, 670001] loss: 0.000002\n",
      "[1, 671001] loss: 0.000003\n",
      "[1, 672001] loss: 0.000002\n",
      "[1, 673001] loss: 0.000002\n",
      "[1, 674001] loss: 0.000002\n",
      "[1, 675001] loss: 0.000003\n",
      "[1, 676001] loss: 0.000002\n",
      "[1, 677001] loss: 0.000002\n",
      "[1, 678001] loss: 0.000002\n",
      "[1, 679001] loss: 0.000002\n",
      "[1, 680001] loss: 0.000002\n",
      "[1, 681001] loss: 0.000003\n",
      "[1, 682001] loss: 0.000002\n",
      "[1, 683001] loss: 0.000002\n",
      "[1, 684001] loss: 0.000002\n",
      "[1, 685001] loss: 0.000002\n",
      "[1, 686001] loss: 0.000002\n",
      "[1, 687001] loss: 0.000002\n",
      "[1, 688001] loss: 0.000002\n",
      "[1, 689001] loss: 0.000002\n",
      "[1, 690001] loss: 0.000002\n",
      "[1, 691001] loss: 0.000002\n",
      "[1, 692001] loss: 0.000002\n",
      "[1, 693001] loss: 0.000002\n",
      "[1, 694001] loss: 0.000002\n",
      "[1, 695001] loss: 0.000003\n",
      "[1, 696001] loss: 0.000002\n",
      "[1, 697001] loss: 0.000003\n",
      "[1, 698001] loss: 0.000002\n",
      "[1, 699001] loss: 0.000002\n",
      "[1, 700001] loss: 0.000002\n",
      "[1, 701001] loss: 0.000003\n",
      "[1, 702001] loss: 0.000002\n",
      "[1, 703001] loss: 0.004416\n",
      "[1, 704001] loss: 0.000002\n",
      "[1, 705001] loss: 0.000002\n",
      "[1, 706001] loss: 0.000002\n",
      "[1, 707001] loss: 0.000002\n",
      "[1, 708001] loss: 0.000002\n",
      "[1, 709001] loss: 0.000002\n",
      "[1, 710001] loss: 0.000002\n",
      "[1, 711001] loss: 0.000002\n",
      "[1, 712001] loss: 0.000002\n",
      "[1, 713001] loss: 0.003635\n",
      "[1, 714001] loss: 0.000002\n",
      "[1, 715001] loss: 0.000003\n",
      "[1, 716001] loss: 0.000002\n",
      "[1, 717001] loss: 0.000002\n",
      "[1, 718001] loss: 0.000002\n",
      "[1, 719001] loss: 0.000002\n",
      "[1, 720001] loss: 0.000003\n",
      "[1, 721001] loss: 0.000002\n",
      "[1, 722001] loss: 0.000002\n",
      "[1, 723001] loss: 0.000002\n",
      "[1, 724001] loss: 0.000003\n",
      "[1, 725001] loss: 0.000002\n",
      "[1, 726001] loss: 0.000002\n",
      "[1, 727001] loss: 0.000002\n",
      "[1, 728001] loss: 0.000002\n",
      "[1, 729001] loss: 0.000002\n",
      "[1, 730001] loss: 0.000003\n",
      "[1, 731001] loss: 0.000002\n",
      "[1, 732001] loss: 0.000002\n",
      "[1, 733001] loss: 0.000002\n",
      "[1, 734001] loss: 0.000002\n",
      "[1, 735001] loss: 0.000002\n",
      "[1, 736001] loss: 0.000002\n",
      "[1, 737001] loss: 0.000002\n",
      "[1, 738001] loss: 0.000003\n",
      "[1, 739001] loss: 0.000002\n",
      "[1, 740001] loss: 0.000002\n",
      "[1, 741001] loss: 0.000002\n",
      "[1, 742001] loss: 0.000002\n",
      "[1, 743001] loss: 0.000002\n",
      "[1, 744001] loss: 0.000003\n",
      "[1, 745001] loss: 0.000002\n",
      "[1, 746001] loss: 0.000003\n",
      "[1, 747001] loss: 0.000002\n",
      "[1, 748001] loss: 0.000002\n",
      "[1, 749001] loss: 0.000002\n",
      "[1, 750001] loss: 0.000003\n",
      "[1, 751001] loss: 0.000002\n",
      "[1, 752001] loss: 0.004329\n",
      "[1, 753001] loss: 0.000002\n",
      "[1, 754001] loss: 0.000002\n",
      "[1, 755001] loss: 0.000002\n",
      "[1, 756001] loss: 0.000002\n",
      "[1, 757001] loss: 0.000002\n",
      "[1, 758001] loss: 0.000002\n",
      "[1, 759001] loss: 0.000002\n",
      "[1, 760001] loss: 0.000002\n",
      "[1, 761001] loss: 0.000002\n",
      "[1, 762001] loss: 0.003608\n",
      "[1, 763001] loss: 0.000002\n",
      "[1, 764001] loss: 0.000003\n",
      "[1, 765001] loss: 0.000002\n",
      "[1, 766001] loss: 0.000002\n",
      "[1, 767001] loss: 0.000002\n",
      "[1, 768001] loss: 0.000003\n",
      "[1, 769001] loss: 0.000003\n",
      "[1, 770001] loss: 0.000002\n",
      "[1, 771001] loss: 0.000003\n",
      "[1, 772001] loss: 0.000002\n",
      "[1, 773001] loss: 0.000003\n",
      "[1, 774001] loss: 0.000002\n",
      "[1, 775001] loss: 0.000002\n",
      "[1, 776001] loss: 0.000002\n",
      "[1, 777001] loss: 0.000002\n",
      "[1, 778001] loss: 0.000002\n",
      "[1, 779001] loss: 0.000003\n",
      "[1, 780001] loss: 0.000002\n",
      "[1, 781001] loss: 0.000002\n",
      "[1, 782001] loss: 0.000002\n",
      "[1, 783001] loss: 0.000002\n",
      "[1, 784001] loss: 0.000002\n",
      "[1, 785001] loss: 0.000002\n",
      "[1, 786001] loss: 0.000002\n",
      "[1, 787001] loss: 0.000003\n",
      "[1, 788001] loss: 0.000002\n",
      "[1, 789001] loss: 0.000002\n",
      "[1, 790001] loss: 0.000002\n",
      "[1, 791001] loss: 0.000002\n",
      "[1, 792001] loss: 0.000002\n",
      "[1, 793001] loss: 0.000003\n",
      "[1, 794001] loss: 0.000002\n",
      "[1, 795001] loss: 0.000003\n",
      "[1, 796001] loss: 0.000002\n",
      "[1, 797001] loss: 0.000002\n",
      "[1, 798001] loss: 0.000002\n",
      "[1, 799001] loss: 0.000003\n",
      "[1, 800001] loss: 0.000002\n",
      "[1, 801001] loss: 0.004248\n",
      "[1, 802001] loss: 0.000002\n",
      "[1, 803001] loss: 0.000002\n",
      "[1, 804001] loss: 0.000002\n",
      "[1, 805001] loss: 0.000002\n",
      "[1, 806001] loss: 0.000002\n",
      "[1, 807001] loss: 0.000002\n",
      "[1, 808001] loss: 0.000002\n",
      "[1, 809001] loss: 0.000002\n",
      "[1, 810001] loss: 0.000002\n",
      "[1, 811001] loss: 0.003580\n",
      "[1, 812001] loss: 0.000002\n",
      "[1, 813001] loss: 0.000003\n",
      "[1, 814001] loss: 0.000002\n",
      "[1, 815001] loss: 0.000002\n",
      "[1, 816001] loss: 0.000002\n",
      "[1, 817001] loss: 0.000003\n",
      "[1, 818001] loss: 0.000003\n",
      "[1, 819001] loss: 0.000002\n",
      "[1, 820001] loss: 0.000003\n",
      "[1, 821001] loss: 0.000002\n",
      "[1, 822001] loss: 0.000003\n",
      "[1, 823001] loss: 0.000002\n",
      "[1, 824001] loss: 0.000002\n",
      "[1, 825001] loss: 0.000002\n",
      "[1, 826001] loss: 0.000002\n",
      "[1, 827001] loss: 0.000002\n",
      "[1, 828001] loss: 0.000003\n",
      "[1, 829001] loss: 0.000002\n",
      "[1, 830001] loss: 0.000002\n",
      "[1, 831001] loss: 0.000002\n",
      "[1, 832001] loss: 0.000002\n",
      "[1, 833001] loss: 0.000002\n",
      "[1, 834001] loss: 0.000002\n",
      "[1, 835001] loss: 0.000003\n",
      "[1, 836001] loss: 0.000003\n",
      "[1, 837001] loss: 0.000002\n",
      "[1, 838001] loss: 0.000002\n",
      "[1, 839001] loss: 0.000002\n",
      "[1, 840001] loss: 0.000002\n",
      "[1, 841001] loss: 0.000002\n",
      "[1, 842001] loss: 0.000003\n",
      "[1, 843001] loss: 0.000002\n",
      "[1, 844001] loss: 0.000003\n",
      "[1, 845001] loss: 0.000002\n",
      "[1, 846001] loss: 0.000002\n",
      "[1, 847001] loss: 0.000002\n",
      "[1, 848001] loss: 0.000003\n",
      "[1, 849001] loss: 0.000002\n",
      "[1, 850001] loss: 0.004179\n",
      "[1, 851001] loss: 0.000002\n",
      "[1, 852001] loss: 0.000002\n",
      "[1, 853001] loss: 0.000002\n",
      "[1, 854001] loss: 0.000002\n",
      "[1, 855001] loss: 0.000002\n",
      "[1, 856001] loss: 0.000002\n",
      "[1, 857001] loss: 0.000002\n",
      "[1, 858001] loss: 0.000002\n",
      "[1, 859001] loss: 0.000003\n",
      "[1, 860001] loss: 0.003558\n",
      "[1, 861001] loss: 0.000002\n",
      "[1, 862001] loss: 0.000003\n",
      "[1, 863001] loss: 0.000002\n",
      "[1, 864001] loss: 0.000002\n",
      "[1, 865001] loss: 0.000002\n",
      "[1, 866001] loss: 0.000002\n",
      "[1, 867001] loss: 0.000003\n",
      "[1, 868001] loss: 0.000002\n",
      "[1, 869001] loss: 0.000002\n",
      "[1, 870001] loss: 0.000002\n",
      "[1, 871001] loss: 0.000003\n",
      "[1, 872001] loss: 0.000002\n",
      "[1, 873001] loss: 0.000002\n",
      "[1, 874001] loss: 0.000002\n",
      "[1, 875001] loss: 0.000002\n",
      "[1, 876001] loss: 0.000002\n",
      "[1, 877001] loss: 0.000003\n",
      "[1, 878001] loss: 0.000002\n",
      "[1, 879001] loss: 0.000002\n",
      "[1, 880001] loss: 0.000002\n",
      "[1, 881001] loss: 0.000002\n",
      "[1, 882001] loss: 0.000002\n",
      "[1, 883001] loss: 0.000002\n",
      "[1, 884001] loss: 0.000003\n",
      "[1, 885001] loss: 0.000003\n",
      "[1, 886001] loss: 0.000002\n",
      "[1, 887001] loss: 0.000002\n",
      "[1, 888001] loss: 0.000002\n",
      "[1, 889001] loss: 0.000002\n",
      "[1, 890001] loss: 0.000002\n",
      "[1, 891001] loss: 0.000003\n",
      "[1, 892001] loss: 0.000002\n",
      "[1, 893001] loss: 0.000003\n",
      "[1, 894001] loss: 0.000002\n",
      "[1, 895001] loss: 0.000002\n",
      "[1, 896001] loss: 0.000002\n",
      "[1, 897001] loss: 0.000003\n",
      "[1, 898001] loss: 0.000002\n",
      "[1, 899001] loss: 0.004114\n",
      "[1, 900001] loss: 0.000002\n",
      "[1, 901001] loss: 0.000002\n",
      "[1, 902001] loss: 0.000002\n",
      "[1, 903001] loss: 0.000002\n",
      "[1, 904001] loss: 0.000002\n",
      "[1, 905001] loss: 0.000002\n",
      "[1, 906001] loss: 0.000002\n",
      "[1, 907001] loss: 0.000002\n",
      "[1, 908001] loss: 0.000003\n",
      "[1, 909001] loss: 0.003536\n",
      "[1, 910001] loss: 0.000002\n",
      "[1, 911001] loss: 0.000003\n",
      "[1, 912001] loss: 0.000002\n",
      "[1, 913001] loss: 0.000002\n",
      "[1, 914001] loss: 0.000002\n",
      "[1, 915001] loss: 0.000002\n",
      "[1, 916001] loss: 0.000003\n",
      "[1, 917001] loss: 0.000002\n",
      "[1, 918001] loss: 0.000002\n",
      "[1, 919001] loss: 0.000002\n",
      "[1, 920001] loss: 0.000003\n",
      "[1, 921001] loss: 0.000002\n",
      "[1, 922001] loss: 0.000002\n",
      "[1, 923001] loss: 0.000002\n",
      "[1, 924001] loss: 0.000002\n",
      "[1, 925001] loss: 0.000002\n",
      "[1, 926001] loss: 0.000003\n",
      "[1, 927001] loss: 0.000002\n",
      "[1, 928001] loss: 0.000002\n",
      "[1, 929001] loss: 0.000002\n",
      "[1, 930001] loss: 0.000002\n",
      "[1, 931001] loss: 0.000002\n",
      "[1, 932001] loss: 0.000002\n",
      "[1, 933001] loss: 0.000003\n",
      "[1, 934001] loss: 0.000003\n",
      "[1, 935001] loss: 0.000002\n",
      "[1, 936001] loss: 0.000002\n",
      "[1, 937001] loss: 0.000002\n",
      "[1, 938001] loss: 0.000002\n",
      "[1, 939001] loss: 0.000002\n",
      "[1, 940001] loss: 0.000003\n",
      "[1, 941001] loss: 0.000002\n",
      "[1, 942001] loss: 0.000003\n",
      "[1, 943001] loss: 0.000002\n",
      "[1, 944001] loss: 0.000002\n",
      "[1, 945001] loss: 0.000002\n",
      "[1, 946001] loss: 0.000003\n",
      "[1, 947001] loss: 0.000002\n",
      "[1, 948001] loss: 0.004054\n",
      "[1, 949001] loss: 0.000002\n",
      "[1, 950001] loss: 0.000002\n",
      "[1, 951001] loss: 0.000002\n",
      "[1, 952001] loss: 0.000002\n",
      "[1, 953001] loss: 0.000002\n",
      "[1, 954001] loss: 0.000002\n",
      "[1, 955001] loss: 0.000002\n",
      "[1, 956001] loss: 0.000002\n",
      "[1, 957001] loss: 0.000002\n",
      "[1, 958001] loss: 0.003515\n",
      "[1, 959001] loss: 0.000002\n",
      "[1, 960001] loss: 0.000003\n",
      "[1, 961001] loss: 0.000002\n",
      "[1, 962001] loss: 0.000002\n",
      "[1, 963001] loss: 0.000002\n",
      "[1, 964001] loss: 0.000002\n",
      "[1, 965001] loss: 0.000003\n",
      "[1, 966001] loss: 0.000002\n",
      "[1, 967001] loss: 0.000002\n",
      "[1, 968001] loss: 0.000002\n",
      "[1, 969001] loss: 0.000003\n",
      "[1, 970001] loss: 0.000002\n",
      "[1, 971001] loss: 0.000002\n",
      "[1, 972001] loss: 0.000002\n",
      "[1, 973001] loss: 0.000002\n",
      "[1, 974001] loss: 0.000002\n",
      "[1, 975001] loss: 0.000003\n",
      "[1, 976001] loss: 0.000002\n",
      "[1, 977001] loss: 0.000002\n",
      "[1, 978001] loss: 0.000002\n",
      "[1, 979001] loss: 0.000002\n",
      "[1, 980001] loss: 0.000002\n",
      "[1, 981001] loss: 0.000002\n",
      "[1, 982001] loss: 0.000002\n",
      "[1, 983001] loss: 0.000003\n",
      "[1, 984001] loss: 0.000002\n",
      "[1, 985001] loss: 0.000002\n",
      "[1, 986001] loss: 0.000002\n",
      "[1, 987001] loss: 0.000002\n",
      "[1, 988001] loss: 0.000002\n",
      "[1, 989001] loss: 0.000003\n",
      "[1, 990001] loss: 0.000002\n",
      "[1, 991001] loss: 0.000003\n",
      "[1, 992001] loss: 0.000002\n",
      "[1, 993001] loss: 0.000002\n",
      "[1, 994001] loss: 0.000002\n",
      "[1, 995001] loss: 0.000002\n",
      "[1, 996001] loss: 0.000002\n",
      "[1, 997001] loss: 0.003999\n",
      "[1, 998001] loss: 0.000002\n",
      "[1, 999001] loss: 0.000002\n",
      "[1, 1000001] loss: 0.000002\n",
      "[1, 1001001] loss: 0.000002\n",
      "[1, 1002001] loss: 0.000002\n",
      "[1, 1003001] loss: 0.000002\n",
      "[1, 1004001] loss: 0.000002\n",
      "[1, 1005001] loss: 0.000002\n",
      "[1, 1006001] loss: 0.000002\n",
      "[1, 1007001] loss: 0.003496\n",
      "[1, 1008001] loss: 0.000002\n",
      "[1, 1009001] loss: 0.000003\n",
      "[1, 1010001] loss: 0.000002\n",
      "[1, 1011001] loss: 0.000002\n",
      "[1, 1012001] loss: 0.000002\n",
      "[1, 1013001] loss: 0.000002\n",
      "[1, 1014001] loss: 0.000003\n",
      "[1, 1015001] loss: 0.000002\n",
      "[1, 1016001] loss: 0.000002\n",
      "[1, 1017001] loss: 0.000002\n",
      "[1, 1018001] loss: 0.000002\n",
      "[1, 1019001] loss: 0.000002\n",
      "[1, 1020001] loss: 0.000002\n",
      "[1, 1021001] loss: 0.000002\n",
      "[1, 1022001] loss: 0.000002\n",
      "[1, 1023001] loss: 0.000002\n",
      "[1, 1024001] loss: 0.000003\n",
      "[1, 1025001] loss: 0.000002\n",
      "[1, 1026001] loss: 0.000002\n",
      "[1, 1027001] loss: 0.000002\n",
      "[1, 1028001] loss: 0.000002\n",
      "[1, 1029001] loss: 0.000002\n",
      "[1, 1030001] loss: 0.000002\n",
      "[1, 1031001] loss: 0.000002\n",
      "[1, 1032001] loss: 0.000002\n",
      "[1, 1033001] loss: 0.000002\n",
      "[1, 1034001] loss: 0.000002\n",
      "[1, 1035001] loss: 0.000002\n",
      "[1, 1036001] loss: 0.000002\n",
      "[1, 1037001] loss: 0.000002\n",
      "[1, 1038001] loss: 0.000003\n",
      "[1, 1039001] loss: 0.000002\n",
      "[1, 1040001] loss: 0.000003\n",
      "[1, 1041001] loss: 0.000002\n",
      "[1, 1042001] loss: 0.000002\n",
      "[1, 1043001] loss: 0.000002\n",
      "[1, 1044001] loss: 0.000002\n",
      "[1, 1045001] loss: 0.000002\n",
      "[1, 1046001] loss: 0.003950\n",
      "[1, 1047001] loss: 0.000002\n",
      "[1, 1048001] loss: 0.000002\n",
      "[1, 1049001] loss: 0.000002\n",
      "[1, 1050001] loss: 0.000002\n",
      "[1, 1051001] loss: 0.000002\n",
      "[1, 1052001] loss: 0.000002\n",
      "[1, 1053001] loss: 0.000002\n",
      "[1, 1054001] loss: 0.000002\n",
      "[1, 1055001] loss: 0.000002\n",
      "[1, 1056001] loss: 0.003478\n",
      "[1, 1057001] loss: 0.000002\n",
      "[1, 1058001] loss: 0.000003\n",
      "[1, 1059001] loss: 0.000002\n",
      "[1, 1060001] loss: 0.000002\n",
      "[1, 1061001] loss: 0.000002\n",
      "[1, 1062001] loss: 0.000002\n",
      "[1, 1063001] loss: 0.000003\n",
      "[1, 1064001] loss: 0.000002\n",
      "[1, 1065001] loss: 0.000002\n",
      "[1, 1066001] loss: 0.000002\n"
     ]
    }
   ],
   "source": [
    "##training the model\n",
    "train_loss, train_acc = [], []\n",
    "\n",
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    train_lossTmp = 0.0\n",
    "    train_accTmp = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_iter, 0):\n",
    "        #print(i)\n",
    "        inputs, labels = data.text, data.label-1 ##data.text = inputs [128x20], data.label = labels [128]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #print(inputs.size())\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.nll_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            print('[{0:d}, {1:5d}] loss: {2:3f}'.format(epoch + 1, i + 1, \n",
    "                                                        running_loss))\n",
    "        train_lossTmp += running_loss\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_accTmp += (predicted == labels).sum().item()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "    train_lossTmp = train_lossTmp/len(train_iter.dataset)\n",
    "    train_acc.append(train_accTmp)\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test\n",
    "correct = 0 \n",
    "total = 0 \n",
    "test_loss, test_acc = [], []\n",
    "\n",
    "model.cpu()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        test_lossTmp = 0.0\n",
    "        \n",
    "        for data in test_iter:\n",
    "            inputs, labels = data.text, data.labels\n",
    "            out = model(inputs)\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss = F.mll_loss(out, labels)\n",
    "            test_lossTmp += loss.item()\n",
    "            \n",
    "        test_lossTmp = test_lossTmp/len(test_iter.dataset)\n",
    "        test_loss.append(test_lossTmp)\n",
    "        \n",
    "        test_accTmp = correct/total\n",
    "        test_acc.append(test_accTmp)\n",
    "        \n",
    "        print('Epoch{0:2d} accuracy of the network on the {1:5d} test images: {2:3f} %%'.format(epoch, \n",
    "                                                                                                total, \n",
    "                                                                                                100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = b, l-1\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = emb(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(20, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.nll_loss(out, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(range(len(word_to_ix), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_max(input):\n",
    "    index = 0\n",
    "    for i in range(len(input), 1):\n",
    "        if input[i] > input[index]:\n",
    "            index = i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_index_max([10,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "Sentence classification in \"positive\" or \"negative\" with CNN (Kim, Y., 2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

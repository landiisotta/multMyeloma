{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Programming\n",
    "\n",
    "## Tensors\n",
    "Tensors are the key of Deep Learning programming, they are the generalization of matrices, that can have more than two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Libraries\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be created from Python lists whith the function `torch.tensor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4])\n",
      "tensor(1)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "##Create a tensor from a list \n",
    "vec = [1, 2, 3, 4]\n",
    "tens = torch.tensor(vec)\n",
    "print(tens)\n",
    "##indexing...\n",
    "print(tens[0])\n",
    "print(tens[0].item()) ##to get a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6]])\n",
      "tensor([ 1,  2,  3])\n"
     ]
    }
   ],
   "source": [
    "##Create a tensor of dimension 2x3\n",
    "mat = [[1, 2, 3], [4, 5, 6]]\n",
    "print(mat)\n",
    "t_mat = torch.tensor(mat)\n",
    "print(t_mat)\n",
    "print(t_mat[0])##to get a tensor vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]]])\n",
      "tensor([[ 1,  2],\n",
      "        [ 3,  4]])\n"
     ]
    }
   ],
   "source": [
    "##Create a 3D tensor of dimensions 2x2x2\n",
    "triD = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n",
    "ten_triD = torch.tensor(triD)\n",
    "print(ten_triD)\n",
    "print(ten_triD[0])##to get a tensor matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main customable object types in tensors are the _long_ format (i.e. integers) and the _float_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8212,  0.4561,  0.1326, -1.3092],\n",
      "        [-0.7630, -0.0695,  0.8082,  0.4521],\n",
      "        [ 0.8198, -0.4683, -2.2071, -0.4712]])\n"
     ]
    }
   ],
   "source": [
    "##Create a random tensor of type float\n",
    "ften = torch.randn(3,4, dtype=torch.float)\n",
    "print(ften)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  1],\n",
      "         [ 1,  1]],\n",
      "\n",
      "        [[ 1,  1],\n",
      "         [ 1,  1]]])\n"
     ]
    }
   ],
   "source": [
    "##Create a  3D tensor of type int\n",
    "iten = torch.ones(2, 2, 2, dtype=torch.long)\n",
    "print(iten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,   2,   3],\n",
      "        [  7,   8,   9],\n",
      "        [  4,   5,   6],\n",
      "        [ 10,  11,  12]])\n",
      "tensor([[  1,   2,   3,   7,   8,   9],\n",
      "        [  4,   5,   6,  10,  11,  12]])\n"
     ]
    }
   ],
   "source": [
    "##All kinds of operations can be made with tensors (e.g. sum...)\n",
    "##Concatenate by row two tensors\n",
    "t1 = torch.tensor([[1, 2, 3], [7, 8, 9]])\n",
    "t2 = torch.tensor([[4, 5, 6], [10, 11, 12]])\n",
    "t = torch.cat([t1, t2])\n",
    "print(t)\n",
    "\n",
    "##Concatenate by column two tensors\n",
    "t3 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t4 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "tt = torch.cat([t3, t4], 1)\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3596,  0.7083,  3.0885, -0.6278, -0.4005, -0.3610, -0.5551,\n",
      "         -0.4536,  0.0187, -1.4297,  0.0323, -0.8992],\n",
      "        [-3.0237, -0.9567, -0.4114,  0.3894,  0.2449,  0.7987,  0.4092,\n",
      "         -0.8265, -0.8696,  0.5213, -1.7497,  0.1060]])\n",
      "tensor([[[ 1.3596,  0.7083,  3.0885, -0.6278],\n",
      "         [-0.4005, -0.3610, -0.5551, -0.4536],\n",
      "         [ 0.0187, -1.4297,  0.0323, -0.8992]],\n",
      "\n",
      "        [[-3.0237, -0.9567, -0.4114,  0.3894],\n",
      "         [ 0.2449,  0.7987,  0.4092, -0.8265],\n",
      "         [-0.8696,  0.5213, -1.7497,  0.1060]]])\n"
     ]
    }
   ],
   "source": [
    "##Start with a tensor of dimension 2x3x4 and reshape it to a tensor 2x12\n",
    "ten234 = torch.randn(2, 3, 4)\n",
    "ten212 = ten234.view(2, 12)\n",
    "print(ten212)\n",
    "print(ten234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graphs and Automatic Differentiation\n",
    "Computation graph is an essential concept for the computation of the backpropagation gradient. \n",
    "When we add two tensors together, what we obtain is an output tensor and the unique information it carries are the data and the dimension. This means that the way in which the output tensor was created (i.e. result of a sum) is lost. If we use the flag `requires_grad=True`, then this information is carried by the output `torch.tensor()` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x7fdb97234a58>\n",
      "<SumBackward0 object at 0x7fdb97234b70>\n"
     ]
    }
   ],
   "source": [
    "##Sum two tensors and keep track of the operation\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float, requires_grad=True)\n",
    "y = torch.tensor([5, 6, 7, 8], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "\n",
    "s = z.sum()\n",
    "print(s.grad_fn) ##once requires_grad is flagged, all the operations are recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "##Now, if we want to compute the partial derivative of s w.r.t x_0,...,x_4\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "None\n",
      "True True\n",
      "<AddBackward1 object at 0x7fdb972a7f28>\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "##Change a requires_grad flag in-place\n",
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "print(x.requires_grad, y.requires_grad) ##as we can observe we haven't keep track of the history\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "print(x.requires_grad, y.requires_grad) ##we have changed the flag in-place\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "print(z.requires_grad)\n",
    "\n",
    "z = z.detach() ##we can erase the history\n",
    "print(z.grad_fn)\n",
    "\n",
    "##if we want to stop autograd to record the history of a tensor with flag equals True\n",
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "y = torch.randn(2, 2, requires_grad=True)\n",
    "z = x + y\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(z.requires_grad) ##this is true, we need to act directly on x and y\n",
    "    print((x ** 2).grad_fn)\n",
    "    print((y ** 2).requires_grad)\n",
    "    print((x + y).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "Core of DL is the _affine_ transformation $f(x) = Ax + b$, where the matrix $A$ and vector $b$ are to be learned.\n",
    "\n",
    "In deep learning the input are the rows of $X$ (n_samples, n_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd80c074510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1) ##set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2889,  0.3574,  0.6554],\n",
      "        [-0.9682,  0.0289,  0.4426]])\n",
      "Linear(in_features=5, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3) ##linear map from R5 to R3\n",
    "x = torch.randn(2, 5) ##the input should have 5 columns since we give the rows as input to the linear function\n",
    "print(lin(x))\n",
    "print(lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since composition of affine transformation is an affine transformation, what we need are non-linear transformations in between the affine layers. The most common non-linear tansformations are: $\\tanh(x)$, $\\sigma(x)$, $ReLU(x)=\\max(0, x)$. These functions have easy-to-compute gradients, this is why are largely used. $\\sigma(x)$ is not used in practice, since its gradient tends to vanish very quickly as the absolute value of the argument grows. Models defaults to $\\tanh(x)$ and $ReLU(x)$.\n",
    "\n",
    "In Pytorch most non-linear functions are in `torch.functional`, which in this case was importe as `F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.2366],\n",
      "        [ 0.2857,  0.6898]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(2, 2)\n",
    "print(F.relu(data)) ##ReLU(x) = max(0, x) [rectified linear unit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Softmax(x)$ function is a non-linear function and usually is the last operation done in a network. It takes in a vector and returns a probability (used for classification tasks).\n",
    "\n",
    "The i-th component of the function is given by: $(Softmax(x))_i=\\frac{\\exp(x_i)}{\\sum_j\\exp(x_j)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3141,  0.2117,  0.1724,  0.3018])\n",
      "tensor(1.)\n",
      "tensor([-1.1581, -1.5525, -1.7578, -1.1981])\n"
     ]
    }
   ],
   "source": [
    "##Let us use Softmax\n",
    "data = torch.randn(4)\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())\n",
    "print(F.log_softmax(data, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to minimize the _loss function_ we need to compute its partial derivatives, with PyTorch, since we can keep track of the objects used to compute it (it is a Tensor), it is straightforward. \n",
    "\n",
    "We can then perform standard gradient updates: $\\theta^{(t+1)}=\\theta^{(t)}-\\eta\\nabla_{\\theta}L(\\theta)$, with $\\theta$ our parameters, $L(\\theta)$ the loss function and $\\eta$ a positive learning rate. This is called the _vanilla gradient update_. Other optimization algorithms (such as Adam...) are available in `torch.optim` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Network\n",
    "All network components should inherit from `nn.Module`, this provides functionality to the components. For example, it makes it keep track of the trainable parameters and swapping between CPU and GPU with `.to(device)`, where the device can be a CPU device (`torch.device('cpu')`) or a CUDA device (`torch.device('cuda')`).\n",
    "\n",
    "### Example: network that takes in a sparse bag-of-words representation and outputs a probability distribution over two labels (\"English\" and \"Spanish\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH'), (['Give', 'it', 'to', 'me'], 'ENGLISH'), (['No', 'creo', 'que', 'sea', 'una', 'buena', 'idea'], 'SPANISH'), (['No', 'it', 'is', 'not', 'a', 'good', 'idea', 'to', 'get', 'lost', 'at', 'sea'], 'ENGLISH')]\n"
     ]
    }
   ],
   "source": [
    "##logistic regression Bag-of-Words classifier (occurrences vector to represent sentences = BoW vector)\n",
    "##if we denote the BOW vector as x, the output of the network will be: log(Softmax(Ax+b))\n",
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 3, 'cafeteria': 5, 'creo': 10, 'si': 24, 'una': 13, 'me': 0, 'idea': 15, 'que': 11, 'comer': 2, 'Yo': 23, 'is': 16, 'on': 25, 'get': 20, 'sea': 12, 'a': 18, 'lost': 21, 'to': 8, 'good': 19, 'buena': 14, 'at': 22, 'it': 7, 'Give': 6, 'not': 17, 'la': 4, 'gusta': 1, 'No': 9}\n"
     ]
    }
   ],
   "source": [
    "##now we need to map each word to a unique integer, which will be its index in the BOWs vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module): #inheriting from nn.Module\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "    \n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1,-1)\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0273, -0.0240,  0.0544,  0.0097,  0.0716, -0.0764, -0.0143,\n",
      "         -0.0177,  0.0284, -0.0008,  0.1714,  0.0610, -0.0730, -0.1184,\n",
      "         -0.0329, -0.0846, -0.0628,  0.0094,  0.1169,  0.1066, -0.1917,\n",
      "          0.1216,  0.0548,  0.1860,  0.1294, -0.1787],\n",
      "        [-0.1865, -0.0946,  0.1722, -0.0327,  0.0839, -0.0911,  0.1924,\n",
      "         -0.0830,  0.1471,  0.0023, -0.1033,  0.1008, -0.1041,  0.0577,\n",
      "         -0.0566, -0.0215, -0.1885, -0.0935,  0.1064, -0.0477,  0.1953,\n",
      "          0.1572, -0.0092, -0.1309,  0.1194,  0.0609]])\n",
      "Parameter containing:\n",
      "tensor([-0.1268,  0.1274])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.]])\n",
      "tensor([[-0.7148, -0.6719]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): ##no need to train\n",
    "    sample = data[0]\n",
    "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(bow_vector)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\":0, \"ENGLISH\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5511, -0.8588]])\n",
      "tensor([[-0.7574, -0.6328]])\n"
     ]
    }
   ],
   "source": [
    "##Let's train the model: we pass instances through to get log probabilities, compute a loss function, \n",
    "##compute the gradient of the loss function and then update the parameters with a gradient step. \n",
    "##As a loss function for our logistic regression we use nn.NLLLoss(), which is the negative log likelihood loss.\n",
    "##NLLLoss wants as input a vector of log probabilities and a target label, this is why we need log_softmax\n",
    "\n",
    "# Run on test data before we train, just to see a before-and-after\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1714, -0.1033])\n"
     ]
    }
   ],
   "source": [
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0944, -2.4075]])\n",
      "tensor([[-2.6246, -0.0752]])\n",
      "tensor([ 0.6117, -0.5436])\n"
     ]
    }
   ],
   "source": [
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "        \n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "Word embeddings are dense vectors of real numbers, one per word in the vocabulary. These vectors are built to preserve the semantic relation between words (i.e. _semantic similarity_), based on the linguistic assumption that words appearing in similar contexts are related to each other, __distributional hypothesis__. Hence, two vectors are close to each other, in terms of a distance, if the word they represent are semantically similar, i.e. appear in the same context. \n",
    "\n",
    "To encode semantic similarity in words we may think up some __semantic attributes__ and give scores to these attributes, e.g.\n",
    "\n",
    "$$\n",
    "q_{mathematician} = [can.run=2.3, likes.coffee=9.4, majored.in.Physics=-5.5,...]\\\\\n",
    "q_{physicist} = [can.run=2.5, likes.coffee=9.1, majored.in.Physics=6.4,...]\n",
    "$$\n",
    "\n",
    "Then, as a measure of similarity:\n",
    "\n",
    "$$\n",
    "Sim(physicist, mathematician) = \\frac{q_{physicist} \\cdot q_{mathematician}}{||q_{physicist}||\\ ||q_{mathematician}||} = \\cos(\\varphi)\n",
    "$$\n",
    "\n",
    "Where, $\\varphi$ is the angle between the two vectors. In this way, extremely similar words (i.e. whose embedding points in the same direction) will have similarity $\\sim 1$, whereas extremely dissimilar words will have similarity $\\sim -1$.\n",
    "\n",
    "Central to the idea of deep learning is that the neural network learns representation of the features, rather then requiring the programmer to design them. Hence, we will have some _latent semantic attributes_ that the network can learn. \n",
    "\n",
    "__Remark__: the output of the model won't be interpretable, if we obtain two embeddings, one for _mathematician_ and the other for _physicist_ with a similar value, let's say, in the second dimension, we are not able to understand what that exactly means.\n",
    "\n",
    "### Word embedding in PyTorch\n",
    "As for the one-hot vectors, we need to define and index for each word, when using embeddings. Embeddings are stored as a $|V|\\times D$ matrix, where $D$ is the dimensionality of the embedding and $|V|$ is the dimension of the vocabulary. \n",
    "\n",
    "What can be done is mapping words to indices and store the 1-to-1 correspondance in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4d880e4510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0])\n",
      "tensor([[-0.8923, -0.0583, -0.1955, -0.9656,  0.4224]])\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"Hello\":0, \"Word\":1} \n",
    "##we now want to obtain the embedding of these two words\n",
    "embeds = nn.Embedding(2, 5) ##we need to specify the vocabulary size, dimensionality of the embedding\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"Hello\"]], dtype=torch.long) ##we are talking of indices, hence we need Int\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram Language Modeling\n",
    "Given a sequence of words $w$ we want to compute $\\mathbb{P}(w_i | w_{i-1}, ..., w_{i-n+1})$, where $w_i$ is the $i$-th word of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range(len(test_sentence)-2)]\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 521.5042]), tensor([ 519.1281]), tensor([ 516.7686]), tensor([ 514.4236]), tensor([ 512.0934]), tensor([ 509.7769]), tensor([ 507.4751]), tensor([ 505.1850]), tensor([ 502.9061]), tensor([ 500.6381])]\n"
     ]
    }
   ],
   "source": [
    "class NGramLanguageModeler(nn.Module): ##in this case the heredited class is nn.Module\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    losses.append(total_loss)\n",
    "print(losses) # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuos Bag-of-Words model\n",
    "Tipically, CBOW is used to quickly train word embeddings and use these embeddings to initialize the embeddings of more complicated models. This process is called _pretraining embeddings_. CBOW tries to predict words given the context of a few word before and a few words after the target word. \n",
    "\n",
    "Given a target word $w_i$ and an $N$ context window on each side of the word, i.e. $C=\\{w_{i-1}, ..., w_{i-N}, ..., w_{i+1}, ..., w_{i+N}\\}$, the model tries to minimize:\n",
    "$$\n",
    "-\\log(p(w_i|C))=-\\log(Softmax(A(\\sum_{w\\in C}q_w)+b))\n",
    "$$\n",
    "where $q_w$ is the embedding for the word $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context] ##word_to_ix is the dictionary with words in text and numbers\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor\n",
    "\n",
    "def get_index_max(input):\n",
    "    index = 0\n",
    "    for i in range(1, len(input)):\n",
    "        if input[i] > input[index]:\n",
    "            index = i\n",
    "        return index\n",
    "    \n",
    "def get_max_prob_result(input, ix_to_word):\n",
    "    return ix_to_word[get_index_max(input)]\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation2 = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view((1, -1))\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation2(out)\n",
    "        return out\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        word = torch.LongTensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "EMDEDDING_DIM = 100\n",
    "\n",
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word_to_ix[word] = i\n",
    "    ix_to_word[i] = word\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with': 0, 'are': 1, 'is': 2, 'a': 4, 'pattern': 5, 'rules': 6, 'program.': 3, 'evolve,': 15, 'We': 7, 'inhabit': 8, 'create': 9, 'computational': 10, 'of': 11, 'directed': 12, 'spirits': 13, 'other': 14, 'computer': 17, 'the': 16, 'they': 18, 'study': 19, 'processes.': 20, 'programs': 21, 'The': 22, 'Computational': 23, 'things': 38, 'evolution': 25, 'direct': 26, 'that': 27, 'spells.': 28, 'process.': 29, 'idea': 30, 'our': 31, 'conjure': 32, 'we': 33, 'by': 34, 'beings': 35, 'effect,': 36, 'computers.': 37, 'data.': 40, 'called': 39, 'about': 41, 'to': 42, 'abstract': 43, 'As': 44, 'manipulate': 45, 'People': 46, 'process': 24, 'processes': 47, 'In': 48}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'with', 1: 'are', 2: 'is', 3: 'program.', 4: 'a', 5: 'pattern', 6: 'rules', 7: 'We', 8: 'inhabit', 9: 'create', 10: 'computational', 11: 'of', 12: 'directed', 13: 'spirits', 14: 'other', 15: 'evolve,', 16: 'the', 17: 'computer', 18: 'they', 19: 'study', 20: 'processes.', 21: 'programs', 22: 'The', 23: 'Computational', 24: 'process', 25: 'evolution', 26: 'direct', 27: 'that', 28: 'spells.', 29: 'process.', 30: 'idea', 31: 'our', 32: 'conjure', 33: 'we', 34: 'by', 35: 'beings', 36: 'effect,', 37: 'computers.', 38: 'things', 39: 'called', 40: 'data.', 41: 'about', 42: 'to', 43: 'abstract', 44: 'As', 45: 'manipulate', 46: 'People', 47: 'processes', 48: 'In'}\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea'), (['the', 'idea', 'a', 'computational'], 'of'), (['idea', 'of', 'computational', 'process.'], 'a'), (['of', 'a', 'process.', 'Computational'], 'computational'), (['a', 'computational', 'Computational', 'processes'], 'process.'), (['computational', 'process.', 'processes', 'are'], 'Computational'), (['process.', 'Computational', 'are', 'abstract'], 'processes'), (['Computational', 'processes', 'abstract', 'beings'], 'are'), (['processes', 'are', 'beings', 'that'], 'abstract'), (['are', 'abstract', 'that', 'inhabit'], 'beings'), (['abstract', 'beings', 'inhabit', 'computers.'], 'that'), (['beings', 'that', 'computers.', 'As'], 'inhabit'), (['that', 'inhabit', 'As', 'they'], 'computers.'), (['inhabit', 'computers.', 'they', 'evolve,'], 'As'), (['computers.', 'As', 'evolve,', 'processes'], 'they'), (['As', 'they', 'processes', 'manipulate'], 'evolve,'), (['they', 'evolve,', 'manipulate', 'other'], 'processes'), (['evolve,', 'processes', 'other', 'abstract'], 'manipulate'), (['processes', 'manipulate', 'abstract', 'things'], 'other'), (['manipulate', 'other', 'things', 'called'], 'abstract'), (['other', 'abstract', 'called', 'data.'], 'things'), (['abstract', 'things', 'data.', 'The'], 'called'), (['things', 'called', 'The', 'evolution'], 'data.'), (['called', 'data.', 'evolution', 'of'], 'The'), (['data.', 'The', 'of', 'a'], 'evolution'), (['The', 'evolution', 'a', 'process'], 'of'), (['evolution', 'of', 'process', 'is'], 'a'), (['of', 'a', 'is', 'directed'], 'process'), (['a', 'process', 'directed', 'by'], 'is'), (['process', 'is', 'by', 'a'], 'directed'), (['is', 'directed', 'a', 'pattern'], 'by'), (['directed', 'by', 'pattern', 'of'], 'a'), (['by', 'a', 'of', 'rules'], 'pattern'), (['a', 'pattern', 'rules', 'called'], 'of'), (['pattern', 'of', 'called', 'a'], 'rules'), (['of', 'rules', 'a', 'program.'], 'called'), (['rules', 'called', 'program.', 'People'], 'a'), (['called', 'a', 'People', 'create'], 'program.'), (['a', 'program.', 'create', 'programs'], 'People'), (['program.', 'People', 'programs', 'to'], 'create'), (['People', 'create', 'to', 'direct'], 'programs'), (['create', 'programs', 'direct', 'processes.'], 'to'), (['programs', 'to', 'processes.', 'In'], 'direct'), (['to', 'direct', 'In', 'effect,'], 'processes.'), (['direct', 'processes.', 'effect,', 'we'], 'In'), (['processes.', 'In', 'we', 'conjure'], 'effect,'), (['In', 'effect,', 'conjure', 'the'], 'we'), (['effect,', 'we', 'the', 'spirits'], 'conjure'), (['we', 'conjure', 'spirits', 'of'], 'the'), (['conjure', 'the', 'of', 'the'], 'spirits'), (['the', 'spirits', 'the', 'computer'], 'of'), (['spirits', 'of', 'computer', 'with'], 'the'), (['of', 'the', 'with', 'our'], 'computer'), (['the', 'computer', 'our', 'spells.'], 'with')]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size, EMBEDDING_DIM)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoc in range(50):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_vector)\n",
    "        loss = loss_function(log_probs, torch.LongTensor([word_to_ix[target]]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n",
      "\n",
      "Context: ['we', 'are', 'to', 'study']\n",
      "\n",
      "Prediction: with\n"
     ]
    }
   ],
   "source": [
    "##TEST\n",
    "context = [\"we\", \"are\", \"to\", \"study\"]\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector).data.numpy()\n",
    "print('Raw text: {}\\n'.format(' '.join(raw_text)))\n",
    "print('Context: {}\\n'.format(context))\n",
    "print('Prediction: {}'.format(get_max_prob_result(a[0], ix_to_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "Sequence models are models where there is some sort of dependence through time between inputs. RNNs are networks that maintain some kind of state. In the LSTM, for each element in the sequence, there is a corresponding _hidden state_ $h_t$, that can contain information from arbitrary points earlier in the sequence. The hidden states can be used to predict words in a language model and other things...\n",
    "\n",
    "PyTorch LSTM expects its inputs to be 3D tensors: (sequence, indexes instances in the mini-batch, indexes elements of the input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4d880e4510>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]])\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]]), tensor([[[-0.9825,  0.4715, -0.0633]]]))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3) ##input and output dim = 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)] ##sequence of length 5\n",
    "\n",
    "##initialize the hidden state\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3)) ##clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

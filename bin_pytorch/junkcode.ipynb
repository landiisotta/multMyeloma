{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding with batch size $1$ and token sequences of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supponiamo di avere tre pazienti con EHRs di lunghezze diverse \n",
    "#vogliamo applicare CBOW considerando il contesto K (user-defined) separatamente per ogni \"sentence\",\n",
    "#cioe' separatamente per ogni paziente\n",
    "#Creiamo one list with different arrays storing the tokens of the medical terms\n",
    "# one array for each patient\n",
    "P1 = \"A wiki is run using wiki software, otherwise known as a wiki engine.\\\n",
    "         A wiki engine is a type of content management system, but it differs\\\n",
    "         from most other such systems, including blog software, in that the\\\n",
    "         content is created without any defined owner or leader, and wikis have\\\n",
    "         little implicit structure, allowing structure to emerge according to the\\\n",
    "         needs of the users.\"\n",
    "\n",
    "P2 = \"The online encyclopedia project Wikipedia is by far the most popular wiki-based\\\n",
    "         website, and is one of the most widely viewed sites of any kind in the world,\\\n",
    "         having been ranked in the top ten since 2007.\"\n",
    "\n",
    "P3 = \"Wikipedia is not a single wiki but rather a collection of hundreds of wikis,\\\n",
    "         one for each language. There are tens of thousands of other wikis in use, both\\\n",
    "         public and private, including wikis functioning as knowledge management resources,\\\n",
    "         notetaking tools, community websites and intranets. The English-language Wikipedia\\\n",
    "         has the largest collection of articles; as of September 2016, it had over five\\\n",
    "         million articles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ARRAY with tokens from patients\n",
    "H = [P1.split(), P2.split(), P3.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fun1: creiamo il nostro dictionary (associamo univocamente un token a un intero)\n",
    "##Input: array of sentences\n",
    "def create_dict(array_sentences): \n",
    "    word_to_ix = {}\n",
    "\n",
    "    for _, j in enumerate(array_sentences):\n",
    "        for word in j:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = create_dict(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[word_to_ix[mt] for mt in a] for _, a in enumerate(H)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(word_to_ix), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAINING\n",
    "EMBEDDING_DIM = 10\n",
    "vocab_size = len(word_to_ix)\n",
    "embedding = nn.Embedding(len(word_to_ix), 10)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(embedding.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-ac896838f9ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "for epoc in range(50):\n",
    "    total_loss = 0\n",
    "    for sequence in data:\n",
    "        embedding.zero_grad()\n",
    "        sequence = torch.tensor(sequence)\n",
    "        log_probs = embedding(sequence)\n",
    "        loss = loss_function(log_probs, torch.LongTensor([word_to_ix[target]]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "        word = torch.LongTensor([word_to_ix[word]])\n",
    "        return embedding(word).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pat_embedding(array_sentences, emb_dim, model):\n",
    "    mat_all = []\n",
    "\n",
    "    for sentence in array_sentences:\n",
    "        emb = torch.empty(len(sentence), emb_dim)\n",
    "        for med_term in sentence:\n",
    "            emb = torch.cat((emb, get_word_embedding(med_term).detach()),0)\n",
    "        mat_all += [emb]\n",
    "\n",
    "    return mat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "mat_all = pat_embedding(H, EMBEDDING_DIM, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([126, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mat_all[0].size())\n",
    "len(H[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4013, -0.6560,  0.0649,  ...,  0.2077,  2.2426, -0.7861],\n",
       "        [ 0.5487,  0.1065,  1.4480,  ...,  0.5269,  1.3526, -0.0994],\n",
       "        [ 0.6229,  0.8686,  0.5328,  ..., -0.0897,  0.2653, -1.1034],\n",
       "        ...,\n",
       "        [-0.6540, -0.4573,  2.6081,  ...,  0.9091, -0.7375, -2.2961],\n",
       "        [-1.8820, -1.5328, -0.0443,  ...,  0.7891, -0.5312, -1.5133],\n",
       "        [ 1.7572, -0.9223,  1.4437,  ...,  0.0981,  1.2850, -0.6266]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _ in enumerate(data):\n",
    "    embedding(torch.LongTensor(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7658, -1.1076, -1.0605,  ..., -0.1243, -0.3123,  0.9120],\n",
       "        [ 0.1745, -0.9270,  0.0739,  ..., -0.9071,  2.3402, -0.2193],\n",
       "        [ 0.4127,  1.6022,  1.6502,  ..., -0.7562,  0.0355, -0.7286],\n",
       "        ...,\n",
       "        [ 1.5872, -0.9021, -0.8099,  ...,  0.1343,  0.4993, -0.6932],\n",
       "        [ 0.3248,  0.2295, -0.8391,  ...,  0.6187,  2.4658, -0.8536],\n",
       "        [ 0.3259, -1.0615,  0.3930,  ...,  0.2402, -0.3682,  0.2493]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
